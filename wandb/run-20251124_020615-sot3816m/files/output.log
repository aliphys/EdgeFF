Running as part of sweep: mjf06338
Sweep override - layers: [784, 200, 200, 200, 200, 10]
Sweep override - softmax_epochs: 10
Sweep override - train_batch_size: 512
Sweep override - seed: 42
[34m[1mwandb[0m: [33mWARNING[0m Config item 'layers' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'seed' was locked by 'sweep' (ignored update).
Found INA3221 power monitor at: /sys/bus/i2c/drivers/ina3221/1-0040/hwmon/hwmon1
Started tegrastats monitoring
Hardware monitoring enabled.
Rep Epoch: 1/10 [512/50000 (1%)]
Rep Epoch: 1/10 [5632/50000 (11%)]
Rep Epoch: 1/10 [10752/50000 (22%)]
Rep Epoch: 1/10 [15872/50000 (32%)]
Rep Epoch: 1/10 [20992/50000 (42%)]
Rep Epoch: 1/10 [26112/50000 (52%)]
Rep Epoch: 1/10 [31232/50000 (62%)]
Rep Epoch: 1/10 [36352/50000 (73%)]
Rep Epoch: 1/10 [41472/50000 (83%)]
Rep Epoch: 1/10 [46592/50000 (93%)]
Rep Epoch: 2/10 [512/50000 (1%)]
Rep Epoch: 2/10 [5632/50000 (11%)]
Rep Epoch: 2/10 [10752/50000 (22%)]
Rep Epoch: 2/10 [15872/50000 (32%)]
Rep Epoch: 2/10 [20992/50000 (42%)]
Rep Epoch: 2/10 [26112/50000 (52%)]
Rep Epoch: 2/10 [31232/50000 (62%)]
Rep Epoch: 2/10 [36352/50000 (73%)]
Rep Epoch: 2/10 [41472/50000 (83%)]
Rep Epoch: 2/10 [46592/50000 (93%)]
Rep Epoch: 3/10 [512/50000 (1%)]
Rep Epoch: 3/10 [5632/50000 (11%)]
Rep Epoch: 3/10 [10752/50000 (22%)]
Rep Epoch: 3/10 [15872/50000 (32%)]
Rep Epoch: 3/10 [20992/50000 (42%)]
Rep Epoch: 3/10 [26112/50000 (52%)]
Rep Epoch: 3/10 [31232/50000 (62%)]
Rep Epoch: 3/10 [36352/50000 (73%)]
Rep Epoch: 3/10 [41472/50000 (83%)]
Rep Epoch: 3/10 [46592/50000 (93%)]
Rep Epoch: 4/10 [512/50000 (1%)]
Rep Epoch: 4/10 [5632/50000 (11%)]
Rep Epoch: 4/10 [10752/50000 (22%)]
Rep Epoch: 4/10 [15872/50000 (32%)]
Rep Epoch: 4/10 [20992/50000 (42%)]
Rep Epoch: 4/10 [26112/50000 (52%)]
Rep Epoch: 4/10 [31232/50000 (62%)]
Rep Epoch: 4/10 [36352/50000 (73%)]
Rep Epoch: 4/10 [41472/50000 (83%)]
Rep Epoch: 4/10 [46592/50000 (93%)]
Rep Epoch: 5/10 [512/50000 (1%)]
Rep Epoch: 5/10 [5632/50000 (11%)]
Rep Epoch: 5/10 [10752/50000 (22%)]
Rep Epoch: 5/10 [15872/50000 (32%)]
Rep Epoch: 5/10 [20992/50000 (42%)]
Rep Epoch: 5/10 [26112/50000 (52%)]
Rep Epoch: 5/10 [31232/50000 (62%)]
Rep Epoch: 5/10 [36352/50000 (73%)]
Rep Epoch: 5/10 [41472/50000 (83%)]
Rep Epoch: 5/10 [46592/50000 (93%)]
Rep Epoch: 6/10 [512/50000 (1%)]
Rep Epoch: 6/10 [5632/50000 (11%)]
Rep Epoch: 6/10 [10752/50000 (22%)]
Rep Epoch: 6/10 [15872/50000 (32%)]
Rep Epoch: 6/10 [20992/50000 (42%)]
Rep Epoch: 6/10 [26112/50000 (52%)]
Rep Epoch: 6/10 [31232/50000 (62%)]
Rep Epoch: 6/10 [36352/50000 (73%)]
Rep Epoch: 6/10 [41472/50000 (83%)]
Rep Epoch: 6/10 [46592/50000 (93%)]
Rep Epoch: 7/10 [512/50000 (1%)]
Rep Epoch: 7/10 [5632/50000 (11%)]
Rep Epoch: 7/10 [10752/50000 (22%)]
Rep Epoch: 7/10 [15872/50000 (32%)]
Rep Epoch: 7/10 [20992/50000 (42%)]
Rep Epoch: 7/10 [26112/50000 (52%)]
Rep Epoch: 7/10 [31232/50000 (62%)]
Rep Epoch: 7/10 [36352/50000 (73%)]
Rep Epoch: 7/10 [41472/50000 (83%)]
Rep Epoch: 7/10 [46592/50000 (93%)]
Rep Epoch: 8/10 [512/50000 (1%)]
Rep Epoch: 8/10 [5632/50000 (11%)]
Rep Epoch: 8/10 [10752/50000 (22%)]
Rep Epoch: 8/10 [15872/50000 (32%)]
Rep Epoch: 8/10 [20992/50000 (42%)]
Rep Epoch: 8/10 [26112/50000 (52%)]
Rep Epoch: 8/10 [31232/50000 (62%)]
Rep Epoch: 8/10 [36352/50000 (73%)]
Rep Epoch: 8/10 [41472/50000 (83%)]
Rep Epoch: 8/10 [46592/50000 (93%)]
Rep Epoch: 9/10 [512/50000 (1%)]
Rep Epoch: 9/10 [5632/50000 (11%)]
Rep Epoch: 9/10 [10752/50000 (22%)]
Rep Epoch: 9/10 [15872/50000 (32%)]
Rep Epoch: 9/10 [20992/50000 (42%)]
Rep Epoch: 9/10 [26112/50000 (52%)]
Rep Epoch: 9/10 [31232/50000 (62%)]
Rep Epoch: 9/10 [36352/50000 (73%)]
Rep Epoch: 9/10 [41472/50000 (83%)]
Rep Epoch: 9/10 [46592/50000 (93%)]
Rep Epoch: 10/10 [512/50000 (1%)]
Rep Epoch: 10/10 [5632/50000 (11%)]
Rep Epoch: 10/10 [10752/50000 (22%)]
Rep Epoch: 10/10 [15872/50000 (32%)]
Rep Epoch: 10/10 [20992/50000 (42%)]
Rep Epoch: 10/10 [26112/50000 (52%)]
Rep Epoch: 10/10 [31232/50000 (62%)]
Rep Epoch: 10/10 [36352/50000 (73%)]
Rep Epoch: 10/10 [41472/50000 (83%)]
Rep Epoch: 10/10 [46592/50000 (93%)]
Softmax Epoch: 1/10 [512/50000 (1%)]
Softmax Epoch: 1/10 [5632/50000 (11%)]
Softmax Epoch: 1/10 [10752/50000 (22%)]
Softmax Epoch: 1/10 [15872/50000 (32%)]
Softmax Epoch: 1/10 [20992/50000 (42%)]
Softmax Epoch: 1/10 [26112/50000 (52%)]
Softmax Epoch: 1/10 [31232/50000 (62%)]
Softmax Epoch: 1/10 [36352/50000 (73%)]
Softmax Epoch: 1/10 [41472/50000 (83%)]
Softmax Epoch: 1/10 [46592/50000 (93%)]
Softmax Epoch: 2/10 [512/50000 (1%)]
Softmax Epoch: 2/10 [5632/50000 (11%)]
Softmax Epoch: 2/10 [10752/50000 (22%)]
Softmax Epoch: 2/10 [15872/50000 (32%)]
Softmax Epoch: 2/10 [20992/50000 (42%)]
Softmax Epoch: 2/10 [26112/50000 (52%)]
Softmax Epoch: 2/10 [31232/50000 (62%)]
Softmax Epoch: 2/10 [36352/50000 (73%)]
Softmax Epoch: 2/10 [41472/50000 (83%)]
Softmax Epoch: 2/10 [46592/50000 (93%)]
Softmax Epoch: 3/10 [512/50000 (1%)]
Softmax Epoch: 3/10 [5632/50000 (11%)]
Softmax Epoch: 3/10 [10752/50000 (22%)]
Softmax Epoch: 3/10 [15872/50000 (32%)]
Softmax Epoch: 3/10 [20992/50000 (42%)]
Softmax Epoch: 3/10 [26112/50000 (52%)]
Softmax Epoch: 3/10 [31232/50000 (62%)]
Softmax Epoch: 3/10 [36352/50000 (73%)]
Softmax Epoch: 3/10 [41472/50000 (83%)]
Softmax Epoch: 3/10 [46592/50000 (93%)]
Softmax Epoch: 4/10 [512/50000 (1%)]
Softmax Epoch: 4/10 [5632/50000 (11%)]
Softmax Epoch: 4/10 [10752/50000 (22%)]
Softmax Epoch: 4/10 [15872/50000 (32%)]
Softmax Epoch: 4/10 [20992/50000 (42%)]
Softmax Epoch: 4/10 [26112/50000 (52%)]
Softmax Epoch: 4/10 [31232/50000 (62%)]
Softmax Epoch: 4/10 [36352/50000 (73%)]
Softmax Epoch: 4/10 [41472/50000 (83%)]
Softmax Epoch: 4/10 [46592/50000 (93%)]
Softmax Epoch: 5/10 [512/50000 (1%)]
Softmax Epoch: 5/10 [5632/50000 (11%)]
Softmax Epoch: 5/10 [10752/50000 (22%)]
Softmax Epoch: 5/10 [15872/50000 (32%)]
Softmax Epoch: 5/10 [20992/50000 (42%)]
Softmax Epoch: 5/10 [26112/50000 (52%)]
Softmax Epoch: 5/10 [31232/50000 (62%)]
Softmax Epoch: 5/10 [36352/50000 (73%)]
Softmax Epoch: 5/10 [41472/50000 (83%)]
Softmax Epoch: 5/10 [46592/50000 (93%)]
Softmax Epoch: 6/10 [512/50000 (1%)]
Softmax Epoch: 6/10 [5632/50000 (11%)]
Softmax Epoch: 6/10 [10752/50000 (22%)]
Softmax Epoch: 6/10 [15872/50000 (32%)]
Softmax Epoch: 6/10 [20992/50000 (42%)]
Softmax Epoch: 6/10 [26112/50000 (52%)]
Softmax Epoch: 6/10 [31232/50000 (62%)]
Softmax Epoch: 6/10 [36352/50000 (73%)]
Softmax Epoch: 6/10 [41472/50000 (83%)]
Softmax Epoch: 6/10 [46592/50000 (93%)]
Softmax Epoch: 7/10 [512/50000 (1%)]
Softmax Epoch: 7/10 [5632/50000 (11%)]
Softmax Epoch: 7/10 [10752/50000 (22%)]
Softmax Epoch: 7/10 [15872/50000 (32%)]
Softmax Epoch: 7/10 [20992/50000 (42%)]
Softmax Epoch: 7/10 [26112/50000 (52%)]
Softmax Epoch: 7/10 [31232/50000 (62%)]
Softmax Epoch: 7/10 [36352/50000 (73%)]
Softmax Epoch: 7/10 [41472/50000 (83%)]
Softmax Epoch: 7/10 [46592/50000 (93%)]
Softmax Epoch: 8/10 [512/50000 (1%)]
Softmax Epoch: 8/10 [5632/50000 (11%)]
Softmax Epoch: 8/10 [10752/50000 (22%)]
Softmax Epoch: 8/10 [15872/50000 (32%)]
Softmax Epoch: 8/10 [20992/50000 (42%)]
Softmax Epoch: 8/10 [26112/50000 (52%)]
Softmax Epoch: 8/10 [31232/50000 (62%)]
Softmax Epoch: 8/10 [36352/50000 (73%)]
Softmax Epoch: 8/10 [41472/50000 (83%)]
Softmax Epoch: 8/10 [46592/50000 (93%)]
Softmax Epoch: 9/10 [512/50000 (1%)]
Softmax Epoch: 9/10 [5632/50000 (11%)]
Softmax Epoch: 9/10 [10752/50000 (22%)]
Softmax Epoch: 9/10 [15872/50000 (32%)]
Softmax Epoch: 9/10 [20992/50000 (42%)]
Softmax Epoch: 9/10 [26112/50000 (52%)]
Softmax Epoch: 9/10 [31232/50000 (62%)]
Softmax Epoch: 9/10 [36352/50000 (73%)]
Softmax Epoch: 9/10 [41472/50000 (83%)]
Softmax Epoch: 9/10 [46592/50000 (93%)]
Softmax Epoch: 10/10 [512/50000 (1%)]
Softmax Epoch: 10/10 [5632/50000 (11%)]
Softmax Epoch: 10/10 [10752/50000 (22%)]
Softmax Epoch: 10/10 [15872/50000 (32%)]
Softmax Epoch: 10/10 [20992/50000 (42%)]
Softmax Epoch: 10/10 [26112/50000 (52%)]
Softmax Epoch: 10/10 [31232/50000 (62%)]
Softmax Epoch: 10/10 [36352/50000 (73%)]
Softmax Epoch: 10/10 [41472/50000 (83%)]
Softmax Epoch: 10/10 [46592/50000 (93%)]

Results for the TRAIN set:
	F1-score: 0.09949782406754308
	Accuracy: 0.09972
	Error: 0.9002799987792969

Inference Metrics for TRAIN set:
	Latency per sample: 0.0016 ms
	Energy per sample: 0.0107 mJ
	Average power: 6655.49 mW

Results for the TEST set:
	F1-score: 0.9408768536901508
	Accuracy: 0.9415
	Error: 0.05849999189376831

Inference Metrics for TEST set:
	Latency per sample: 0.0015 ms
	Energy per sample: 0.0102 mJ
	Average power: 6615.87 mW

Results for the VALIDATION set:
	F1-score: 0.9334211507853596
	Accuracy: 0.9344
	Error: 0.06559997797012329

Inference Metrics for VALIDATION set:
	Latency per sample: 0.0016 ms
	Energy per sample: 0.0106 mJ
	Average power: 6734.72 mW
Averaged mean:  7.872182836226384
Averaged std:  2.398016946776109
Averaged mean_all_incorrect_labels:  -6.300691408163526
Averaged std_all_incorrect_labels:  3.1907183573909297
Averaged mean:  10.604579621501957
Averaged std:  2.9120426411152613
Averaged mean_all_incorrect_labels:  -6.134894048492469
Averaged std_all_incorrect_labels:  3.4842603217842627
Averaged mean:  15.969488602615465
Averaged std:  3.8803310907036304
Averaged mean_all_incorrect_labels:  -6.133344167098828
Averaged std_all_incorrect_labels:  4.423503944970029
Averaged mean:  17.75036036010895
Averaged std:  4.2472062201442435
Averaged mean_all_incorrect_labels:  -8.461669270699906
Averaged std_all_incorrect_labels:  4.91838664500278
Averaged mean:  19.90993731032776
Averaged std:  4.658839836467463
Averaged mean_all_incorrect_labels:  -5.94921524921554
Averaged std_all_incorrect_labels:  5.125835130457589
  0%|                                                                                 | 0/10000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/jetson/Documents/github/EdgeFF/exploratory/Refractoring_gpu/Main.py", line 410, in <module>
    main()
  File "/home/jetson/Documents/github/EdgeFF/exploratory/Refractoring_gpu/Main.py", line 384, in main
    Evaluation.eval_val_set_light(model, inputs=test_inputs_full, targets=test_targets_full,
  File "/home/jetson/Documents/github/EdgeFF/exploratory/Refractoring_gpu/Evaluation.py", line 91, in eval_val_set_light
    y_predicted[chunk_indices_validation[i]], predicted_with_layers_up_to[chunk_indices_validation[i]] = \
  File "/home/jetson/.local/lib/python3.10/site-packages/torch/_tensor.py", line 1213, in __array__
    return self.numpy().astype(dtype, copy=False)
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
Traceback (most recent call last):
  File "/home/jetson/Documents/github/EdgeFF/exploratory/Refractoring_gpu/Main.py", line 410, in <module>
    main()
  File "/home/jetson/Documents/github/EdgeFF/exploratory/Refractoring_gpu/Main.py", line 384, in main
    Evaluation.eval_val_set_light(model, inputs=test_inputs_full, targets=test_targets_full,
  File "/home/jetson/Documents/github/EdgeFF/exploratory/Refractoring_gpu/Evaluation.py", line 91, in eval_val_set_light
    y_predicted[chunk_indices_validation[i]], predicted_with_layers_up_to[chunk_indices_validation[i]] = \
  File "/home/jetson/.local/lib/python3.10/site-packages/torch/_tensor.py", line 1213, in __array__
    return self.numpy().astype(dtype, copy=False)
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
