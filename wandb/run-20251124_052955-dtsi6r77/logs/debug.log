2025-11-24 05:29:55,670 INFO    MainThread:91635 [wandb_setup.py:_flush():80] Current SDK version is 0.23.0
2025-11-24 05:29:55,670 INFO    MainThread:91635 [wandb_setup.py:_flush():80] Configure stats pid to 91635
2025-11-24 05:29:55,670 INFO    MainThread:91635 [wandb_setup.py:_flush():80] Loading settings from /home/jetson/.config/wandb/settings
2025-11-24 05:29:55,670 INFO    MainThread:91635 [wandb_setup.py:_flush():80] Loading settings from /home/jetson/Documents/github/EdgeFF/wandb/settings
2025-11-24 05:29:55,670 INFO    MainThread:91635 [wandb_setup.py:_flush():80] Loading settings from environment variables
2025-11-24 05:29:55,670 INFO    MainThread:91635 [wandb_init.py:setup_run_log_directory():713] Logging user logs to /home/jetson/Documents/github/EdgeFF/wandb/run-20251124_052955-dtsi6r77/logs/debug.log
2025-11-24 05:29:55,671 INFO    MainThread:91635 [wandb_init.py:setup_run_log_directory():714] Logging internal logs to /home/jetson/Documents/github/EdgeFF/wandb/run-20251124_052955-dtsi6r77/logs/debug-internal.log
2025-11-24 05:29:55,671 INFO    MainThread:91635 [wandb_init.py:init():840] calling init triggers
2025-11-24 05:29:55,671 INFO    MainThread:91635 [wandb_init.py:init():845] wandb.init called with sweep_config: {'layers': '784,100,100,10', 'rep-epochs': 10, 'seed': 42, 'softmax-epochs': 10, 'train-batch-size': 512}
config: {'layers': [784, 100, 100, 10], 'representation_epochs': 10, 'softmax_epochs': 10, 'train_batch_size': 512, 'test_batch_size': 512, 'val_size': 10000, 'log_val_samples': 1000, 'final_train_sample': 5000, 'final_val_sample': 2000, 'seed': 42, 'dataset': 'MNIST', 'train_flag': True, 'enable_hw_monitor': True, 'hw_interval_ms': 500, 'device': 'cpu', 'cuda_available': True, 'cuda_device_name': None, '_wandb': {}}
2025-11-24 05:29:55,671 INFO    MainThread:91635 [wandb_init.py:init():888] starting backend
2025-11-24 05:29:55,911 INFO    MainThread:91635 [wandb_init.py:init():891] sending inform_init request
2025-11-24 05:29:55,915 INFO    MainThread:91635 [wandb_init.py:init():899] backend started and connected
2025-11-24 05:29:55,917 INFO    MainThread:91635 [wandb_run.py:_config_callback():1385] config_cb None None {'layers': '784,100,100,10', 'rep-epochs': 10, 'seed': 42, 'softmax-epochs': 10, 'train-batch-size': 512}
2025-11-24 05:29:55,918 INFO    MainThread:91635 [wandb_init.py:init():969] updated telemetry
2025-11-24 05:29:55,925 INFO    MainThread:91635 [wandb_init.py:init():993] communicating run to backend with 600.0 second timeout
2025-11-24 05:29:56,595 INFO    MainThread:91635 [wandb_init.py:init():1040] starting run threads in backend
2025-11-24 05:29:56,810 INFO    MainThread:91635 [wandb_run.py:_console_start():2504] atexit reg
2025-11-24 05:29:56,811 INFO    MainThread:91635 [wandb_run.py:_redirect():2352] redirect: wrap_raw
2025-11-24 05:29:56,811 INFO    MainThread:91635 [wandb_run.py:_redirect():2421] Wrapping output streams.
2025-11-24 05:29:56,811 INFO    MainThread:91635 [wandb_run.py:_redirect():2444] Redirects installed.
2025-11-24 05:29:56,815 INFO    MainThread:91635 [wandb_init.py:init():1080] run started, returning control to user process
2025-11-24 05:29:56,820 INFO    MainThread:91635 [wandb_run.py:_config_callback():1385] config_cb None None {'representation_epochs': 10, 'softmax_epochs': 10, 'train_batch_size': 512, 'device': 'cpu', 'cuda_available': True, 'cuda_device_name': None}
2025-11-24 05:42:34,369 INFO    MainThread:91635 [wandb_run.py:_finish():2270] finishing run aliphys-lu/SweepCUDA100+200/dtsi6r77
2025-11-24 05:42:34,371 INFO    MainThread:91635 [wandb_run.py:_atexit_cleanup():2469] got exitcode: 0
2025-11-24 05:42:34,371 INFO    MainThread:91635 [wandb_run.py:_restore():2451] restore
2025-11-24 05:42:34,371 INFO    MainThread:91635 [wandb_run.py:_restore():2457] restore done
2025-11-24 05:42:35,478 INFO    MainThread:91635 [wandb_run.py:_footer_sync_info():3853] logging synced files
